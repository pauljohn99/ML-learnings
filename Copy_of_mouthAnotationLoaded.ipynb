{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4VEEdZJlDbcykOjXP7HU4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauljohn99/ML-learnings/blob/main/Copy_of_mouthAnotationLoaded.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pycocotools --quiet\n",
        "# !git clone https://github.com/pytorch/vision.git\n",
        "# !git checkout v0.3.0\n",
        "# !cp vision/references/detection/utils.py ./\n",
        "# !cp vision/references/detection/transforms.py ./\n",
        "# !cp vision/references/detection/coco_eval.py ./\n",
        "# !cp vision/references/detection/engine.py ./\n",
        "# !cp vision/references/detection/coco_utils.py ./"
      ],
      "metadata": {
        "id": "S9CkR1nvEtuE"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import random\n",
        "import numpy as np\n",
        "# import pandas as pd\n",
        "# for ignoring warnings\n",
        "# import warnings\n",
        "# warnings.filterwarnings('ignore')\n",
        "\n",
        "# We will be reading images using OpenCV\n",
        "import cv2\n",
        "\n",
        "# xml library for parsing xml files\n",
        "# from xml.etree import ElementTree as et\n",
        "\n",
        "# matplotlib for visualization\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.patches as patches\n",
        "\n",
        "# torchvision libraries\n",
        "import torch\n",
        "import torchvision\n",
        "# from torchvision import transforms as torchtrans  \n",
        "# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# these are the helper libraries imported.\n",
        "# from engine import train_one_epoch, evaluate\n",
        "# import utils\n",
        "# import transforms as T\n",
        "\n",
        "# import requests\n",
        "\n",
        "# for image augmentations\n",
        "# import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2"
      ],
      "metadata": {
        "id": "SSr04RQ0HjxZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "d9QggXGaHtD4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = '/content/clear3.jpg'"
      ],
      "metadata": {
        "id": "NGyUQTqdH_0m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "Hv1WbbniJ3r3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"/content/mouth_anotation.pt\",map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "tAFDfk_fJShw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "    \n",
        "    # torchvision returns the indices of the bboxes to keep\n",
        "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "    \n",
        "    final_prediction = orig_prediction\n",
        "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
        "    \n",
        "    return final_prediction\n",
        "\n",
        "# function to convert a torchtensor back to PIL image\n",
        "# def torch_to_pil(img):\n",
        "#     return torchtrans.ToPILImage()(img).convert('RGB')"
      ],
      "metadata": {
        "id": "jAm5Dm2pL877"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convertor(img, width, height,transforms=None):\n",
        "  img = cv2.imread(img)\n",
        "       \n",
        "  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "  img_res = cv2.resize(img_rgb, (width,height), cv2.INTER_AREA)\n",
        "        # diving by 255\n",
        "  img_res /= 255.0\n",
        "  if transforms:\n",
        "            \n",
        "            sample = transforms(image = img_res)\n",
        "            \n",
        "            img_res = sample['image']\n",
        "  return img_res"
      ],
      "metadata": {
        "id": "yjSChW5xIchS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mouth_anotation(test_dir):\n",
        " test_dataset = convertor(test_dir, 480, 480, ToTensorV2(p=1.0))\n",
        "# pick one image from the test set\n",
        "#  y=len(test_dataset)\n",
        " model.eval()\n",
        "#  for i in range(y):\n",
        " img= test_dataset\n",
        "#  # put the model in evaluation mode\n",
        "#  model.eval()\n",
        " with torch.no_grad():\n",
        "  prediction = model([img])[0]\n",
        "  # i=i+1  \n",
        "#  print('EXPECTED OUTPUT\\n')\n",
        "#  plot_img_bbox(torch_to_pil(img), target)\n",
        " print('MODEL OUTPUT\\n')\n",
        " nms_prediction = apply_nms(prediction, iou_thresh=0.1)\n",
        "#  print(nms_prediction['scores'])\n",
        "#  print(nms_prediction['scores'].detach().cpu().numpy())\n",
        " p=nms_prediction['scores'].detach().cpu().numpy()\n",
        " pr=np.argmax(p)\n",
        " x=max(p)\n",
        " print(x)\n",
        " print(nms_prediction)\n",
        " pred=np.reshape(nms_prediction['boxes'][pr].detach().cpu().numpy(), (-1, 4))\n",
        " print(pred[0])\n",
        " \n",
        " pred1={'boxes': pred}\n",
        " print(pred1)\n",
        " pre=pred[0]\n",
        " print(pre[0],pre[1],pre[2]-pre[0],pre[3]-pre[1])\n",
        "\n",
        "#  print('NMS APPLIED MODEL OUTPUT')\n",
        "#  plot_img_bbox(torch_to_pil(img), pred1)"
      ],
      "metadata": {
        "id": "sxU1MGJKLTap"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_dataset = MouthImagesDataset(test_dir, 480, 480, ToTensorV2(p=1.0))\n",
        "mouth_anotation(test_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPsbgn09UbFz",
        "outputId": "9997b9aa-0815-4e63-8bb9-f982c0223936"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL OUTPUT\n",
            "\n",
            "0.36550716\n",
            "{'boxes': tensor([[188.3082, 222.5484, 267.1335, 249.4854],\n",
            "        [ 66.9184,  55.5149, 343.4340, 405.3907]]), 'labels': tensor([1, 1]), 'scores': tensor([0.3655, 0.1264])}\n",
            "[188.30817 222.54839 267.13354 249.48544]\n",
            "{'boxes': array([[188.30817, 222.54839, 267.13354, 249.48544]], dtype=float32)}\n",
            "188.30817 222.54839 78.82538 26.937057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def plot_img_bbox(img, target):\n",
        "#     # plot the image and bboxes\n",
        "#     # Bounding boxes are defined as follows: x-min y-min width height\n",
        "#     fig, a = plt.subplots(1,1)\n",
        "#     fig.set_size_inches(5,5)\n",
        "#     a.imshow(img)\n",
        "#     for box in (target['boxes']):\n",
        "#         x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n",
        "#         rect = patches.Rectangle((x, y),\n",
        "#                                  width, height,\n",
        "#                                  linewidth = 2,\n",
        "#                                  edgecolor = 'g',\n",
        "#                                  facecolor = 'none')\n",
        "\n",
        "#         # Draw the bounding box on top of the image\n",
        "#         print(type(rect))\n",
        "#         a.add_patch(rect)\n",
        "#     plt.show()\n",
        "    \n",
        "# # plotting the image with bboxes. Feel free to change the index\n",
        "# # img, target = dataset[0]\n",
        "# # plot_img_bbox(img, target)"
      ],
      "metadata": {
        "id": "A1NBaWs4OqzM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class MouthImagesDataset:\n",
        "\n",
        "#     def __init__(self,img, width, height, transforms=None):\n",
        "#         self.transforms = transforms\n",
        "#         # self.files_dir = files_dir\n",
        "#         self.height = height\n",
        "#         self.width = width\n",
        "        \n",
        "#         # sorting the images for consistency\n",
        "#         # To get images, the extension of the filename is checked to be jpg\n",
        "#         self.imgs = img\n",
        "        \n",
        "          \n",
        "#         # classes: 0 index is reserved for background\n",
        "#         self.classes = [_,'mouth']\n",
        "\n",
        "#     def __str__(self):\n",
        "\n",
        "#         # img_name = self.imgs[idx]\n",
        "#         # image_path = os.path.join(self.files_dir, img_name)\n",
        "#         image_path=self.imgs\n",
        "\n",
        "#         # reading the images and converting them to correct size and color    \n",
        "#         img = cv2.imread(image_path)\n",
        "       \n",
        "#         img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "#         img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "#         # diving by 255\n",
        "#         img_res /= 255.0\n",
        "# #         # annotation file\n",
        "# #         annot_filename = img_name[:-4] + '.xml'\n",
        "# #         annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
        "        \n",
        "# #         boxes = []\n",
        "# #         labels = []\n",
        "# #         tree = et.parse(annot_file_path)\n",
        "# #         root = tree.getroot()\n",
        "        \n",
        "# #         # cv2 image gives size as height x width\n",
        "# #         wt = img.shape[1]\n",
        "# #         ht = img.shape[0]\n",
        "        \n",
        "# #         # box coordinates for xml files are extracted and corrected for image size given\n",
        "# #         for member in root.findall('object'):\n",
        "# #             labels.append(self.classes.index(member.find('name').text))\n",
        "            \n",
        "# #             # bounding box\n",
        "# #             xmin = int(member.find('bndbox').find('xmin').text)\n",
        "# #             xmax = int(member.find('bndbox').find('xmax').text)\n",
        "            \n",
        "# #             ymin = int(member.find('bndbox').find('ymin').text)\n",
        "# #             ymax = int(member.find('bndbox').find('ymax').text)\n",
        "            \n",
        "            \n",
        "# #             xmin_corr = (xmin/wt)*self.width\n",
        "# #             xmax_corr = (xmax/wt)*self.width\n",
        "# #             ymin_corr = (ymin/ht)*self.height\n",
        "# #             ymax_corr = (ymax/ht)*self.height\n",
        "            \n",
        "# #             boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
        "        \n",
        "# #         # convert boxes into a torch.Tensor\n",
        "# #         boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        \n",
        "# #         # getting the areas of the boxes\n",
        "# #         area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "# #         # suppose all instances are not crowd\n",
        "# #         iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "        \n",
        "# #         labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "\n",
        "# #         target = {}\n",
        "# #         target[\"boxes\"] = boxes\n",
        "# #         target[\"labels\"] = labels\n",
        "# #         target[\"area\"] = area\n",
        "# #         target[\"iscrowd\"] = iscrowd\n",
        "# #         # image_id\n",
        "# #         image_id = torch.tensor([idx])\n",
        "# #         target[\"image_id\"] = image_id\n",
        "\n",
        "\n",
        "#         if self.transforms:\n",
        "            \n",
        "#             sample = self.transforms(image = img_res,\n",
        "#                                      )\n",
        "            \n",
        "#             img_res = sample['image']\n",
        "# #             target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "            \n",
        "            \n",
        "            \n",
        "#         # return img_res, target\n",
        "#         return img_res\n",
        "#     def __len__(self):\n",
        "#         return len(self.imgs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zq_FObMeQaTw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Send train=True fro training transforms and False for val/test transforms\n",
        "# def get_transform(train):\n",
        "    \n",
        "#     if train:\n",
        "#         return A.Compose([\n",
        "#                             A.HorizontalFlip(0.5),\n",
        "#                      # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "#                             ToTensorV2(p=1.0) \n",
        "#                         ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "#     else:\n",
        "#         return A.Compose([\n",
        "#                             ToTensorV2(p=1.0)\n",
        "#                         ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "metadata": {
        "id": "i-_Hc2mwbI0b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def mouth_anotation(test_dataset):\n",
        "#  test_dataset = convertor(test_dataset, 480, 480, ToTensorV2(p=1.0))\n",
        "# # pick one image from the test set\n",
        "#  y=len(test_dataset)\n",
        "#  model.eval()\n",
        "#  for i in range(y):\n",
        "#   img= test_dataset[i]\n",
        "# #  # put the model in evaluation mode\n",
        "# #  model.eval()\n",
        "#   with torch.no_grad():\n",
        "#     prediction = model([img.to(device)])[0]\n",
        "#   i=i+1  \n",
        "# #  print('EXPECTED OUTPUT\\n')\n",
        "# #  plot_img_bbox(torch_to_pil(img), target)\n",
        "#   print('MODEL OUTPUT\\n')\n",
        "#   nms_prediction = apply_nms(prediction, iou_thresh=0.1)\n",
        "# #  print(nms_prediction['scores'])\n",
        "# #  print(nms_prediction['scores'].detach().cpu().numpy())\n",
        "#   p=nms_prediction['scores'].detach().cpu().numpy()\n",
        "#   pr=np.argmax(p)\n",
        "#   x=max(p)\n",
        "#   print(x)\n",
        "#   print(nms_prediction)\n",
        "#   pred=np.reshape(nms_prediction['boxes'][pr].detach().cpu().numpy(), (-1, 4))\n",
        "#   print(pred[0])\n",
        "# #   pred1={'boxes': pred}\n",
        "# #   print(pred1)\n",
        "# # #  print('NMS APPLIED MODEL OUTPUT')\n",
        "# #   plot_img_bbox(torch_to_pil(img), pred1)"
      ],
      "metadata": {
        "id": "Nsu18jLIKNeB"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}