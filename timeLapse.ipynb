{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7bVMJSsLv59tWJzZDQDmr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauljohn99/ML-learnings/blob/main/timeLapse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycocotools --quiet\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "!git checkout v0.3.0\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/transforms.py ./\n",
        "!cp vision/references/detection/coco_eval.py ./\n",
        "!cp vision/references/detection/engine.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9CkR1nvEtuE",
        "outputId": "e48d98ad-b428-4bc4-c517-b30ec1d12eae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'vision' already exists and is not an empty directory.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for ignoring warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# We will be reading images using OpenCV\n",
        "import cv2\n",
        "\n",
        "# xml library for parsing xml files\n",
        "from xml.etree import ElementTree as et\n",
        "\n",
        "# matplotlib for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# torchvision libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms as torchtrans  \n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# these are the helper libraries imported.\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "# for image augmentations\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2"
      ],
      "metadata": {
        "id": "SSr04RQ0HjxZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9QggXGaHtD4",
        "outputId": "f4e900e5-9fd8-42ad-8745-c06cc8b9304e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = '/content/drive/MyDrive/mouthOpenClosed/closed'"
      ],
      "metadata": {
        "id": "NGyUQTqdH_0m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "Hv1WbbniJ3r3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"/content/mouth_model.pt\",map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "tAFDfk_fJShw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_nms(orig_prediction, iou_thresh=0.3):\n",
        "    \n",
        "    # torchvision returns the indices of the bboxes to keep\n",
        "    keep = torchvision.ops.nms(orig_prediction['boxes'], orig_prediction['scores'], iou_thresh)\n",
        "    \n",
        "    final_prediction = orig_prediction\n",
        "    final_prediction['boxes'] = final_prediction['boxes'][keep]\n",
        "    final_prediction['scores'] = final_prediction['scores'][keep]\n",
        "    final_prediction['labels'] = final_prediction['labels'][keep]\n",
        "    \n",
        "    return final_prediction\n",
        "\n",
        "# function to convert a torchtensor back to PIL image\n",
        "def torch_to_pil(img):\n",
        "    return torchtrans.ToPILImage()(img).convert('RGB')"
      ],
      "metadata": {
        "id": "jAm5Dm2pL877"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FruitImagesDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, files_dir, width, height, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.files_dir = files_dir\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        \n",
        "        # sorting the images for consistency\n",
        "        # To get images, the extension of the filename is checked to be jpg\n",
        "        self.imgs = [image for image in sorted(os.listdir(files_dir))\n",
        "                        if image[-4:]=='.jpg']\n",
        "        \n",
        "          \n",
        "        # classes: 0 index is reserved for background\n",
        "        self.classes = [_,'mouth']\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_name = self.imgs[idx]\n",
        "        image_path = os.path.join(self.files_dir, img_name)\n",
        "\n",
        "        # reading the images and converting them to correct size and color    \n",
        "        img = cv2.imread(image_path)\n",
        "       \n",
        "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "        img_res = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n",
        "        # diving by 255\n",
        "        img_res /= 255.0\n",
        "#         # annotation file\n",
        "#         annot_filename = img_name[:-4] + '.xml'\n",
        "#         annot_file_path = os.path.join(self.files_dir, annot_filename)\n",
        "        \n",
        "#         boxes = []\n",
        "#         labels = []\n",
        "#         tree = et.parse(annot_file_path)\n",
        "#         root = tree.getroot()\n",
        "        \n",
        "#         # cv2 image gives size as height x width\n",
        "#         wt = img.shape[1]\n",
        "#         ht = img.shape[0]\n",
        "        \n",
        "#         # box coordinates for xml files are extracted and corrected for image size given\n",
        "#         for member in root.findall('object'):\n",
        "#             labels.append(self.classes.index(member.find('name').text))\n",
        "            \n",
        "#             # bounding box\n",
        "#             xmin = int(member.find('bndbox').find('xmin').text)\n",
        "#             xmax = int(member.find('bndbox').find('xmax').text)\n",
        "            \n",
        "#             ymin = int(member.find('bndbox').find('ymin').text)\n",
        "#             ymax = int(member.find('bndbox').find('ymax').text)\n",
        "            \n",
        "            \n",
        "#             xmin_corr = (xmin/wt)*self.width\n",
        "#             xmax_corr = (xmax/wt)*self.width\n",
        "#             ymin_corr = (ymin/ht)*self.height\n",
        "#             ymax_corr = (ymax/ht)*self.height\n",
        "            \n",
        "#             boxes.append([xmin_corr, ymin_corr, xmax_corr, ymax_corr])\n",
        "        \n",
        "#         # convert boxes into a torch.Tensor\n",
        "#         boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        \n",
        "#         # getting the areas of the boxes\n",
        "#         area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "#         # suppose all instances are not crowd\n",
        "#         iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
        "        \n",
        "#         labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "\n",
        "#         target = {}\n",
        "#         target[\"boxes\"] = boxes\n",
        "#         target[\"labels\"] = labels\n",
        "#         target[\"area\"] = area\n",
        "#         target[\"iscrowd\"] = iscrowd\n",
        "#         # image_id\n",
        "#         image_id = torch.tensor([idx])\n",
        "#         target[\"image_id\"] = image_id\n",
        "\n",
        "\n",
        "        if self.transforms:\n",
        "            \n",
        "            sample = self.transforms(image = img_res,\n",
        "                                     )\n",
        "            \n",
        "            img_res = sample['image']\n",
        "#             target['boxes'] = torch.Tensor(sample['bboxes'])\n",
        "            \n",
        "            \n",
        "            \n",
        "        # return img_res, target\n",
        "        return img_res\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zq_FObMeQaTw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Send train=True fro training transforms and False for val/test transforms\n",
        "# def get_transform(train):\n",
        "    \n",
        "#     if train:\n",
        "#         return A.Compose([\n",
        "#                             A.HorizontalFlip(0.5),\n",
        "#                      # ToTensorV2 converts image to pytorch tensor without div by 255\n",
        "#                             ToTensorV2(p=1.0) \n",
        "#                         ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "#     else:\n",
        "#         return A.Compose([\n",
        "#                             ToTensorV2(p=1.0)\n",
        "#                         ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "metadata": {
        "id": "i-_Hc2mwbI0b"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = FruitImagesDataset(test_dir, 480, 480, ToTensorV2(p=1.0))\n",
        "mouth_anotation(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPsbgn09UbFz",
        "outputId": "f8920f79-aca9-4a8a-f6cf-c0ddea35102d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL OUTPUT\n",
            "\n",
            "0.99808526\n",
            "{'boxes': tensor([[133.8947, 191.7812, 388.3915, 269.0995]]), 'labels': tensor([1]), 'scores': tensor([0.9981])}\n",
            "[133.89473 191.78117 388.39148 269.09946]\n",
            "MODEL OUTPUT\n",
            "\n",
            "0.9956233\n",
            "{'boxes': tensor([[ 27.7956, 154.8666, 407.4683, 307.5329]]), 'labels': tensor([1]), 'scores': tensor([0.9956])}\n",
            "[ 27.795557 154.86656  407.4683   307.53293 ]\n",
            "MODEL OUTPUT\n",
            "\n",
            "0.9907189\n",
            "{'boxes': tensor([[124.7398, 191.6214, 356.3762, 289.4250]]), 'labels': tensor([1]), 'scores': tensor([0.9907])}\n",
            "[124.73984 191.62138 356.37616 289.42505]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mouth_anotation(test_dataset):\n",
        " test_dataset = FruitImagesDataset(test_dir, 480, 480, ToTensorV2(p=1.0))\n",
        "# pick one image from the test set\n",
        " y=len(test_dataset)\n",
        " model.eval()\n",
        " for i in range(3):\n",
        "  img= test_dataset[i]\n",
        "#  # put the model in evaluation mode\n",
        "#  model.eval()\n",
        "  with torch.no_grad():\n",
        "    prediction = model([img.to(device)])[0]\n",
        "  i=i+1  \n",
        "#  print('EXPECTED OUTPUT\\n')\n",
        "#  plot_img_bbox(torch_to_pil(img), target)\n",
        "  print('MODEL OUTPUT\\n')\n",
        "  nms_prediction = apply_nms(prediction, iou_thresh=0.1)\n",
        "#  print(nms_prediction['scores'])\n",
        "#  print(nms_prediction['scores'].detach().cpu().numpy())\n",
        "  p=nms_prediction['scores'].detach().cpu().numpy()\n",
        "  pr=np.argmax(p)\n",
        "  x=max(p)\n",
        "  print(x)\n",
        "  print(nms_prediction)\n",
        "  pred=np.reshape(nms_prediction['boxes'][pr].detach().cpu().numpy(), (-1, 4))\n",
        "  print(pred[0])\n",
        "#   pred1={'boxes': pred}\n",
        "#   print(pred1)\n",
        "# #  print('NMS APPLIED MODEL OUTPUT')\n",
        "#   plot_img_bbox(torch_to_pil(img), pred1)"
      ],
      "metadata": {
        "id": "Nsu18jLIKNeB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install mediapipe"
      ],
      "metadata": {
        "id": "H8Njv9jTiXEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install imageio-ffmpeg"
      ],
      "metadata": {
        "id": "kRjyMNRpijjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install imageio==2.4.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FG5mZjhi257",
        "outputId": "0b9726d1-8a65-47ed-f2f7-f8802e80468d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imageio==2.4.1 in /usr/local/lib/python3.8/dist-packages (2.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from imageio==2.4.1) (1.21.6)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from imageio==2.4.1) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install moviepy"
      ],
      "metadata": {
        "id": "VyMd52ETlC0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vnbGCtoNEV4k"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import math\n",
        "\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "\n",
        "from moviepy.editor import ImageClip, concatenate_videoclips, CompositeVideoClip\n",
        "from moviepy.video.fx.all import crop\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detector Params\n",
        "NUM_FACE = 1\n",
        "MIN_DETECTION_CONFIDENCE = 0.05\n",
        "MIN_TRACKING_CONFIDENCE = 0.05\n",
        "# Load the face landmark model from mediapipe\n",
        "mpFaceMesh = mp.solutions.face_mesh\n",
        "faceMesh = mpFaceMesh.FaceMesh(max_num_faces=NUM_FACE, min_detection_confidence=MIN_DETECTION_CONFIDENCE, min_tracking_confidence=MIN_TRACKING_CONFIDENCE)\n",
        "frames =(\"/content/drive/MyDrive/mouthOpenClosed/closed/1-closed.png\") \n",
        "crop_smile(frames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioohehOTl_18",
        "outputId": "7a6ca679-22df-420c-f8be-3b684e2e46dd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MODEL OUTPUT\n",
            "\n",
            "0.99808526\n",
            "{'boxes': tensor([[133.8947, 191.7812, 388.3915, 269.0995]]), 'labels': tensor([1]), 'scores': tensor([0.9981])}\n",
            "[133.89473 191.78117 388.39148 269.09946]\n",
            "MODEL OUTPUT\n",
            "\n",
            "0.9956233\n",
            "{'boxes': tensor([[ 27.7956, 154.8666, 407.4683, 307.5329]]), 'labels': tensor([1]), 'scores': tensor([0.9956])}\n",
            "[ 27.795557 154.86656  407.4683   307.53293 ]\n",
            "MODEL OUTPUT\n",
            "\n",
            "0.9907189\n",
            "{'boxes': tensor([[124.7398, 191.6214, 356.3762, 289.4250]]), 'labels': tensor([1]), 'scores': tensor([0.9907])}\n",
            "[124.73984 191.62138 356.37616 289.42505]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_video(frames, output_file=\"out.mp4\"):\n",
        "\tif len(frames) < 2:\n",
        "\t\tlogging.warning(\"Not enough frames to create video.\")\n",
        "\t\treturn (0, \"Not enough frames to create video.\")\n",
        "\n",
        "\tvideo_width = 1280\n",
        "\tvideo_height = 720\n",
        "\n",
        "\t# Crop images to smiles\n",
        "\tcrop_coordinates = [crop_smile(i) for i in frames]\n",
        "\n",
        "\t# Add images\n",
        "\timageclips = [ImageClip(m).set_duration(2).crossfadein(1).set_start(i) for i,m in enumerate(frames)]\n",
        "\t# Crop images\n",
        "\timageclips = [crop(m, *crop_coordinates[i]) for i,m in enumerate(imageclips) if crop_coordinates[i] is not None]\n",
        "\n",
        "\tif len(imageclips) < 2:\n",
        "\t\tlogging.warning(\"Not enough frames with teeth detected to create video.\")\n",
        "\t\treturn (0, \"Not enough frames with teeth detected to create video.\")\n",
        "\n",
        "\t# Resize to fit\n",
        "\timageclips = [i.resize(video_height/i.size[0] if i.size[0] > i.size[1] else video_width/i.size[1]) for i in imageclips]\n",
        "\t# Re-position to center\n",
        "\timageclips = [i.set_position(\"center\") for i in imageclips]\n",
        "\n",
        "\twatermark = ImageClip(\"assets/watermark.png\").set_duration(len(imageclips)+1).crossfadein(1).set_start(0)\n",
        "\twatermark = watermark.resize(video_width/(2*watermark.size[0]))\n",
        "\twatermark = watermark.set_position((0.4, 0.8), relative=\"true\")\n",
        "\n",
        "\timageclips.append(watermark)\n",
        "\n",
        "\tconcat_clip = CompositeVideoClip(imageclips, size=(video_width,video_height))\n",
        "\tconcat_clip.write_videofile(output_file, fps=24, codec=\"libx264\")\n",
        "\n",
        "\treturn (1, \"Success\")\n",
        "\n",
        "def crop_smile(img_file):\n",
        "\timg = cv2.imread(img_file)\n",
        "\tpadding_factor = 0.4\n",
        "\tsc = mouth_anotation(img)\n",
        "\tif sc is None:\n",
        "\t\treturn None\n",
        "\t(x, y, w, h) = sc\n",
        "\ty = math.floor(y-padding_factor*h if (padding_factor*h <= y) else 0)\n",
        "\tx = math.floor(x-padding_factor*w if (padding_factor*w <= x) else 0)\n",
        "\tw = math.ceil(w*(1+2*padding_factor))\n",
        "\th = math.ceil(h*(1+2*padding_factor))\n",
        "\treturn (x, y, x+w, y+h)\n",
        "\n",
        "\n",
        "def detect_smile(img):\n",
        "\tsmile_img = img.copy()\n",
        "\timage_width = smile_img.shape[0]\n",
        "\timage_height = smile_img.shape[1]\n",
        "\timgRGB = cv2.cvtColor(smile_img, cv2.COLOR_BGR2RGB)\n",
        "\tresults = faceMesh.process(imgRGB)\n",
        "\tif results.multi_face_landmarks:\n",
        "\t\tfaceLms = results.multi_face_landmarks[0]\n",
        "\t\tleft_corner = (faceLms.landmark[61].x*image_width, faceLms.landmark[61].y*image_height)\n",
        "\t\ttop_corner = (faceLms.landmark[0].x*image_width, faceLms.landmark[0].y*image_height)\n",
        "\t\tright_corner = (faceLms.landmark[291].x*image_width, faceLms.landmark[291].y*image_height)\n",
        "\t\tbottom_corner = (faceLms.landmark[17].x*image_width, faceLms.landmark[17].y*image_height)\n",
        "\n",
        "\t\treturn (left_corner[0], top_corner[1], right_corner[0]-left_corner[0], bottom_corner[1]-top_corner[1])\n",
        "\telse:\n",
        "\t\treturn None"
      ],
      "metadata": {
        "id": "VARuDbX_kD8o"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}